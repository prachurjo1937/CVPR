{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/github-rokon/CVPR/blob/main/Face_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow pyheif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjknnjuzPkLm",
        "outputId": "07d67d4a-312f-45a7-dd48-78afc34043c3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Requirement already satisfied: pyheif in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyheif) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.0->pyheif) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CW60c3ZJjkzP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vcfSqMBuJ1n-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
        "\n",
        "\n",
        "import pyheif\n",
        "import cv2\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOGi0jjqKrr4",
        "outputId": "e208c518-0873-4fee-ec98-c23f16709185"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/Dataset'"
      ],
      "metadata": {
        "id": "JMkldW68K1MQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')"
      ],
      "metadata": {
        "id": "i5QmT3C5dWHM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKpmI0dvlsNA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_heic_image(image_path):\n",
        "    # Read HEIC image using pyheif\n",
        "    heif_file = pyheif.read(image_path)\n",
        "    # Convert it into a format that Pillow can work with\n",
        "    image = Image.frombytes(\n",
        "        heif_file.mode,\n",
        "        heif_file.size,\n",
        "        heif_file.data,\n",
        "        \"raw\",\n",
        "        heif_file.mode,\n",
        "        heif_file.stride,\n",
        "    )\n",
        "    return image"
      ],
      "metadata": {
        "id": "UStnELZkQCJF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "source": [
        "# Load dataset with resizing images to the same dimensions\n",
        "def load_dataset(dataset_path):\n",
        "    images = []\n",
        "    labels = []\n",
        "    label_names = os.listdir(dataset_path)\n",
        "\n",
        "    for index, name in enumerate(tqdm(label_names)):\n",
        "        folder_path = os.path.join(dataset_path, name)\n",
        "        # Check if the current item is a directory\n",
        "        if os.path.isdir(folder_path):\n",
        "            for image_name in os.listdir(folder_path):\n",
        "                image_path = os.path.join(folder_path, image_name)\n",
        "\n",
        "                # Check if the file is a HEIC or regular image\n",
        "                if image_name.lower().endswith('.heic'):\n",
        "                    try:\n",
        "                        image = load_heic_image(image_path)  # Load HEIC image\n",
        "                        image = np.array(image)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading HEIC image: {image_path} - {e}\")\n",
        "                        continue\n",
        "                else:\n",
        "                    image = cv2.imread(image_path)  # Load non-HEIC image with OpenCV\n",
        "\n",
        "                if image is not None:  # Check if the image was successfully loaded\n",
        "                    image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
        "                    images.append(image)\n",
        "                    labels.append(index)\n",
        "                else:\n",
        "                    print(f\"Warning: Could not load image {image_path}\")\n",
        "        else:\n",
        "            print(f\"Skipping non-directory: {folder_path}\")\n",
        "\n",
        "    images = np.array(images, dtype='float32') / 255.0  # Normalize\n",
        "    labels = np.array(labels)\n",
        "    labels = to_categorical(labels, num_classes=len(label_names))\n",
        "\n",
        "    return images, labels, label_names # Return the images and labels\n",
        "\n",
        "# Example usage\n",
        "dataset_path = '/content/drive/MyDrive/Dataset' # Replace with the actual path to your dataset\n",
        "images, labels, label_names = load_dataset(dataset_path) # Call the function and assign the returned values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTEsinBspJel",
        "outputId": "d0066e2c-c59a-4ad5-d867-c45be9eeecd2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1/58 [00:00<00:32,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not load image /content/drive/MyDrive/Dataset/Rokon/Dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 45/58 [00:34<00:02,  4.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading HEIC image: /content/drive/MyDrive/Dataset/SUN/IMG_6943.HEIC - Input is not a HEIF/AVIF file\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58/58 [00:40<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping non-directory: /content/drive/MyDrive/Dataset/face_recognition_model.keras\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "\n",
        "# Test different camera indices\n",
        "for i in range(5):  # Try indices 0 to 4\n",
        "    cap = cv2.VideoCapture(i)\n",
        "    if cap.isOpened():\n",
        "        print(f\"Camera found at index {i}\")\n",
        "        cap.release()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i99FpvgnmFEv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build CNN model\n",
        "def build_model(num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dense(num_classes, activation='softmax'),\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes=len(label_names))\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "GCnfhrUxQ_Sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b15f2c5-a3bc-4b9a-d12a-40714c2c0551"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPFG3rPxRIy8",
        "outputId": "1ea0952c-36a7-4e65-e846-47b7374a9edf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 7s/step - accuracy: 0.0318 - loss: 4.2393 - val_accuracy: 0.0115 - val_loss: 4.1173\n",
            "Epoch 2/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 0.1225 - loss: 3.7001 - val_accuracy: 0.1494 - val_loss: 3.8612\n",
            "Epoch 3/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 7s/step - accuracy: 0.3367 - loss: 2.9358 - val_accuracy: 0.2069 - val_loss: 3.6463\n",
            "Epoch 4/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 7s/step - accuracy: 0.5885 - loss: 2.1936 - val_accuracy: 0.2874 - val_loss: 3.3317\n",
            "Epoch 5/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 7s/step - accuracy: 0.7980 - loss: 1.1938 - val_accuracy: 0.3448 - val_loss: 3.0200\n",
            "Epoch 6/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7s/step - accuracy: 0.9298 - loss: 0.5681 - val_accuracy: 0.3793 - val_loss: 2.8829\n",
            "Epoch 7/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 8s/step - accuracy: 0.9638 - loss: 0.2455 - val_accuracy: 0.4598 - val_loss: 2.6609\n",
            "Epoch 8/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 7s/step - accuracy: 0.9940 - loss: 0.1063 - val_accuracy: 0.4138 - val_loss: 2.9034\n",
            "Epoch 9/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0413 - val_accuracy: 0.4368 - val_loss: 2.8808\n",
            "Epoch 10/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 7s/step - accuracy: 0.9978 - loss: 0.0157 - val_accuracy: 0.4483 - val_loss: 2.7704\n",
            "Epoch 11/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0091 - val_accuracy: 0.4253 - val_loss: 2.8157\n",
            "Epoch 12/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 0.4598 - val_loss: 2.8832\n",
            "Epoch 13/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0042 - val_accuracy: 0.4713 - val_loss: 2.9752\n",
            "Epoch 14/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0034 - val_accuracy: 0.4713 - val_loss: 3.0277\n",
            "Epoch 15/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.4598 - val_loss: 3.0283\n",
            "Epoch 16/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0020 - val_accuracy: 0.4598 - val_loss: 3.0149\n",
            "Epoch 17/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0019 - val_accuracy: 0.4483 - val_loss: 3.0175\n",
            "Epoch 18/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0017 - val_accuracy: 0.4598 - val_loss: 3.0189\n",
            "Epoch 19/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 0.4598 - val_loss: 3.0310\n",
            "Epoch 20/20\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 7s/step - accuracy: 1.0000 - loss: 0.0013 - val_accuracy: 0.4483 - val_loss: 3.0473\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def real_time_recognition(model, label_names):\n",
        "    # Initialize the webcam (use 0 as the parameter to select the default webcam)\n",
        "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # For Windows\n",
        "\n",
        "    # Check if the webcam is opened correctly\n",
        "    if not cap.isOpened():\n",
        "        raise IOError(\"Cannot open webcam\")\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()  # Read a frame from the webcam\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Convert the frame to grayscale (Haar cascades work with grayscale images)\n",
        "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Draw a rectangle around each face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "\n",
        "        # Extract the face region\n",
        "        face_region = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face region for prediction\n",
        "        face = cv2.resize(face_region, (224, 224))\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "        face = face / 255.0\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(face)\n",
        "        predicted_class = label_names[np.argmax(prediction)]\n",
        "\n",
        "        # Display the predicted class label\n",
        "        cv2.putText(frame, predicted_class, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Show the frame with face rectangles and predicted class labels\n",
        "    cv2.imshow('Real-time Face Recognition', frame)\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "\n",
        "    # Release the webcam and close all OpenCV windows\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Assuming 'model' is your trained model and 'label_names' is a list of class names\n",
        "# real_time_recognition(model, label_names)"
      ],
      "metadata": {
        "id": "gznKCyWNXgSS"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}